{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis in textual movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset\")\n",
    "\n",
    "from glob import glob\n",
    "filenames_neg = sorted(glob(op.join('data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "def open_perso(f):\n",
    "    with open(f, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "texts_neg = [open_perso(f) for f in filenames_neg]\n",
    "texts_pos = [open_perso(f) for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Complete the count_words function that will count the number of occurrences of each distinct word in a list of string and return vocabulary (the python dictionary) and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts, stop_words = []):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "        \n",
    "    stop_words : unused here. (Only for compatibily with Q.5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples = len(texts)\n",
    "    \n",
    "    # Determine n_features and delete punctuation.\n",
    "    words = dict()\n",
    "    n_features = 0\n",
    "    texts_sub = []\n",
    "    for text in texts:\n",
    "        texts_sub.append(re.sub(\"[\\n\\r\\-\\_\\@\\$\\&,:;.!?'\\\"]\", \" \", text))\n",
    "        \n",
    "        for word in texts_sub[-1].split(\" \"):\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            if word not in words:\n",
    "                words[word] = n_features\n",
    "                n_features += 1\n",
    "    \n",
    "    \n",
    "    # Count words by documents and store it in an array.\n",
    "    counts = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for word in texts_sub[i].split(\" \"):\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            j = words[word]\n",
    "            counts[i,j] += 1\n",
    "    \n",
    "    return words, counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explain how positive and negative classes have been assigned to movie reviews (see poldata.README.2.0 file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class has been assigned thanks to explicit words/rules. For instance : (\"8/10\", \"four out of five\") are some explicit pattern recognize. Then given a note, class can be assigned : for example, with a five-star system, three-and-a-half stars and up are considered positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Complete the NBclass to implement the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of the vocab done here is not optimal. We have to compute a new one for each X given to predict y. It would probably be better to cut the count_words implemented here in two part, one to compute the vocab, and the other one to count the words given a vocab.\n",
    "\n",
    "Moreover one could use the **counts** array as X for the classifier rather than **texts**. (Some operations could be faster with the optimized np arrays) Specially for computing the cross validation score. We could transform once texts in counts (as counts is what takes most time to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Naive Bayes classifier.\"\"\"\n",
    "    def __init__(self, stop_words = []):\n",
    "        self.vocab = dict()\n",
    "        self.classes = dict()\n",
    "        self.classes_inv = dict() # Useful to invert classes at the end\n",
    "        self.class_probability = np.array([])\n",
    "        self.word_prob_in_class = np.array([])\n",
    "        \n",
    "        # Used in Q.5\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        assert len(X) == len(Y), \"Sizes of X and Y don't match.\"\n",
    "        \n",
    "        # Link a class to an index\n",
    "        i = 0\n",
    "        for cls in Y:\n",
    "            if cls not in self.classes:\n",
    "                self.classes[cls] = i\n",
    "                self.classes_inv[i] = cls\n",
    "                i += 1\n",
    "        \n",
    "        \n",
    "        # P(cls = c) for all c\n",
    "        self.class_probability = np.zeros(len(self.classes))\n",
    "        \n",
    "        for cls in Y:\n",
    "            self.class_probability[self.classes[cls]] += 1\n",
    "        self.class_probability /= len(Y)\n",
    "        \n",
    "        self.vocab, counts = count_words(X, self.stop_words)\n",
    "        \n",
    "        # P(word = w_i | cls = c) for all w_i, c\n",
    "        # Smoothing : start with 1.\n",
    "        self.word_prob_in_class = np.ones((len(self.classes), len(self.vocab)))\n",
    "        \n",
    "        for j in range(len(self.vocab)):\n",
    "            for i in range(len(X)):\n",
    "                cls_ind = self.classes[Y[i]]\n",
    "                self.word_prob_in_class[cls_ind, j] += counts[i, j]\n",
    "        \n",
    "        # Normalize each line (class)\n",
    "        self.word_prob_in_class /= np.sum(self.word_prob_in_class, axis = 1).reshape((len(self.classes), 1))        \n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        vocab, counts = count_words(X, self.stop_words)\n",
    "        \n",
    "        scores = np.ones((len(X), len(self.classes))) * np.log(self.class_probability)\n",
    "        \n",
    "        #for i in range(len(X)): ## NOT OPTI\n",
    "        #    for w in vocab:\n",
    "        #        scores[i] += counts[i,vocab[w]] * np.log(self.word_prob_in_class[:,self.vocab[w]])\n",
    "        \n",
    "        for w in vocab: ## BETTER\n",
    "            ind_1 = vocab[w]\n",
    "            ind_2 = self.vocab.get(w, -1)\n",
    "            if (ind_2 == -1):\n",
    "                continue\n",
    "            scores += np.dot(counts[:,ind_1:ind_1+1], np.log(self.word_prob_in_class[:, ind_2:ind_2+1]).T)\n",
    "        \n",
    "        result = np.argmax(scores, axis = 1)\n",
    "        \n",
    "        for i in range(len(result)):\n",
    "            result[i] = self.classes_inv[result[i]]\n",
    "        return result\n",
    "            \n",
    "\n",
    "    def score(self, X, Y):\n",
    "        return np.mean(self.predict(X) == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on the train set : 0.9655\n"
     ]
    }
   ],
   "source": [
    "print(\"Score on the train set :\", NB().fit(texts, y).score(texts, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the performance of your classifier in cross-validation 5-folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(NB(), texts, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score on a 5-fold CV :  0.8115\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean score on a 5-fold CV : \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Change the count_words function to ignore the “stop words” in the file data/english.stop. Are the performances improved ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts, stop_words):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    stop_words : list of str\n",
    "        The words to ignore\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples = len(texts)\n",
    "    \n",
    "    # Determine n_features and delete punctuation.\n",
    "    words = dict()\n",
    "    n_features = 0\n",
    "    texts_sub = []\n",
    "    for text in texts:\n",
    "        texts_sub.append(re.sub(\"[\\n\\r\\-\\_\\@\\$\\&,:;.!?'\\\"]\", \" \", text))\n",
    "        \n",
    "        for word in texts_sub[-1].split(\" \"):\n",
    "            if word == \"\" or (word in stop_words):\n",
    "                continue\n",
    "            if word not in words:\n",
    "                words[word] = n_features\n",
    "                n_features += 1\n",
    "    \n",
    "    \n",
    "    # Count words by documents and store it in an array.\n",
    "    counts = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for word in texts_sub[i].split(\" \"):\n",
    "            if word == \"\" or word in stop_words:\n",
    "                continue\n",
    "            j = words[word]\n",
    "            counts[i,j] += 1\n",
    "    \n",
    "    return words, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/english.stop\", \"r\") as f:\n",
    "    stop_words = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on the train set : 0.976\n"
     ]
    }
   ],
   "source": [
    "print(\"Score on the train set :\", NB(stop_words).fit(texts, y).score(texts, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score on a 5-fold CV :  0.8019999999999999\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB(stop_words), texts, y, cv=5)\n",
    "print(\"Mean score on a 5-fold CV : \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performances aren't improved when we delete the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compare your implementation with scikitlearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "bayes_classifier = Pipeline([(\"Vectorize\", vectorizer), (\"Classify\", clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on the train set: 0.97\n"
     ]
    }
   ],
   "source": [
    "print(\"Score on the train set:\", bayes_classifier.fit(texts,y).score(texts, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score on a 5-fold CV :  0.812\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(bayes_classifier, texts, y, cv=5)\n",
    "print(\"Mean score on a 5-fold CV : \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With char "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score on a 5-fold CV :  0.6094999999999999\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "vectorizer = CountVectorizer(analyzer = 'char')\n",
    "\n",
    "bayes_classifier = Pipeline([(\"Vectorize\", vectorizer), (\"Classify\", clf)])\n",
    "\n",
    "scores = cross_val_score(bayes_classifier, texts, y, cv=5)\n",
    "print(\"Mean score on a 5-fold CV : \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn is much quicker to compute. (But one can notice that their 'count words' methods is divided in two (fit and transform). I still believe that we should do the same for the part I). \n",
    "\n",
    "And it gives around the same result as we have in part I. We can see that counting the character is far less efficient that the words. That makes sense as we lost the meaning of words when we do such a thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test another classification methodscikitlearn(ex : LinearSVC, LogisticRegression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score on a 5-fold CV :  0.8325000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(max_iter = 20000)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "svc_classifier = Pipeline([(\"Vectorize\", vectorizer), (\"Classify\", clf)])\n",
    "\n",
    "scores = cross_val_score(svc_classifier, texts, y, cv=5)\n",
    "print(\"Mean score on a 5-fold CV : \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performances are a bit better with a svm model. But it takes a bit more time to compute than the Naive Bayes Classifier with sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use NLTK library in order to process a stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "\n",
    "class Counter:\n",
    "    \"\"\"Do the same as count_words but including a stemmer and separating the fit from the transform (as in sklearn).\"\"\"\n",
    "    \n",
    "    def __init__(self, stop_words = None, stemmer = None):\n",
    "        self.vocab = dict()\n",
    "        self.n_features = 0\n",
    "        \n",
    "        self.stop_words = []\n",
    "        if stop_words:\n",
    "            self.stop_words = stop_words\n",
    "        self.stemmer = stemmer\n",
    "        \n",
    "    def fit(self, texts, *args, **kwargs):\n",
    "        self.vocab = dict()\n",
    "        self.n_features = 0\n",
    "        \n",
    "        # Determine n_features, vocab.\n",
    "        # Delete punctuation.\n",
    "        for text in texts:\n",
    "            text = re.sub(\"[\\n\\r\\-\\_\\@\\$\\&,:;.!?'\\\"]\", \" \", text)\n",
    "\n",
    "            for word in text.split(\" \"):\n",
    "                if self.stemmer:\n",
    "                    word = self.stemmer.stem(word)\n",
    "                \n",
    "                if word == \"\" or word in self.stop_words:\n",
    "                    continue\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = self.n_features\n",
    "                    self.n_features += 1\n",
    "        return self\n",
    "                    \n",
    "    def transform(self, texts, *args, **kwargs):\n",
    "        # Count words by documents and store it in an array.\n",
    "        n_samples = len(texts)\n",
    "        counts = np.zeros((n_samples, self.n_features)) # Could use sparse matrix as sklearn ?\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            text = re.sub(\"[\\n\\r\\-\\_\\@\\$\\&,:;.!?'\\\"]\", \" \", texts[i])\n",
    "            for word in text.split(\" \"):\n",
    "                if self.stemmer:\n",
    "                    word = self.stemmer.stem(word)\n",
    "                \n",
    "                if word == \"\" or word in self.stop_words or word not in self.vocab:\n",
    "                    continue\n",
    "                j = self.vocab[word]\n",
    "                counts[i,j] += 1\n",
    "        return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score on a 5-fold CV :  0.8099999999999999\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(stemmer = SnowballStemmer(\"english\"))\n",
    "clf = MultinomialNB()\n",
    "\n",
    "bayes_classifier = Pipeline([(\"Vectorize\", counter), (\"Classify\", clf)])\n",
    "\n",
    "scores = cross_val_score(bayes_classifier, texts, y, cv=5)\n",
    "print(\"Mean score on a 5-fold CV : \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming hasn't improve the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Filter words by grammatical category (POS : Part Of Speech) and keep only nouns,verbs, adverbs and adjectives for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "class Counter:\n",
    "    \"\"\"Do the same as count_words but keep only the right grammatical category \n",
    "    Also separating the fit from the transform (as in sklearn).\"\"\"\n",
    "    \n",
    "    def __init__(self, stop_words = None):\n",
    "        self.vocab = dict()\n",
    "        self.n_features = 0\n",
    "        \n",
    "        self.stop_words = []\n",
    "        if stop_words:\n",
    "            self.stop_words = stop_words\n",
    "            \n",
    "    def fit(self, texts, *args, **kwargs):\n",
    "        self.vocab = dict()\n",
    "        self.n_features = 0\n",
    "        \n",
    "        # Determine n_features, vocab.\n",
    "        # Delete punctuation.\n",
    "        for text in texts:\n",
    "            text = re.sub(\"[\\n\\r\\-\\_\\@\\$\\&'\\\"]\", \" \", text)\n",
    "            fltr = map(lambda x:x[0], filter(lambda x: x[1][0] in \"NVRJ\", pos_tag(list(filter(lambda x: x!=\"\", text.split(\" \"))))))\n",
    "            for word in fltr:\n",
    "                if word == \"\" or word in self.stop_words:\n",
    "                    continue\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = self.n_features\n",
    "                    self.n_features += 1\n",
    "\n",
    "        return self\n",
    "                    \n",
    "    def transform(self, texts, *args, **kwargs):\n",
    "        # Count words by documents and store it in an array.\n",
    "        n_samples = len(texts)\n",
    "        counts = np.zeros((n_samples, self.n_features)) # Could use sparse matrix as sklearn ?\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            text = re.sub(\"[\\n\\r\\-\\_\\@\\$\\&'\\\"]\", \" \", texts[i])\n",
    "            fltr = map(lambda x:x[0], filter(lambda x: x[1][0] in \"NVRJ\", pos_tag(list(filter(lambda x: x != \"\", text.split(\" \"))))))\n",
    "            for word in fltr:                \n",
    "                if word == \"\" or word in self.stop_words or word not in self.vocab:\n",
    "                    continue\n",
    "                j = self.vocab[word]\n",
    "                counts[i,j] += 1\n",
    "        return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score on a 5-fold CV :  0.8074999999999999\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "clf = MultinomialNB()\n",
    "\n",
    "bayes_classifier = Pipeline([(\"Vectorize\", counter), (\"Classify\", clf)])\n",
    "\n",
    "scores = cross_val_score(bayes_classifier, texts, y, cv=5, error_score= 'raise')\n",
    "print(\"Mean score on a 5-fold CV : \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also doesn't improve the result. And it takes much more time. We should also work with counts rather than text to optimize computational time !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
